import os
import json
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score
import xgboost as xgb
from google.colab import drive

drive.mount('/content/drive')
data_path = '/content/drive/My Drive/AI_Security_Project/EMBER2018'

# Step 2: Feature Extraction Function
def extract_features(sample):
    features = []
    features.extend(sample['histogram'])
    features.extend(sample['byteentropy'])
    features.append(sample['strings']['numstrings'])
    features.append(sample['strings']['avlength'])
    features.append(sample['strings']['printables'])
    features.append(sample['strings']['entropy'])
    features.append(sample['strings']['paths'])
    features.append(sample['strings']['urls'])
    features.append(sample['strings']['registry'])
    features.append(sample['strings']['MZ'])
    features.append(sample['general']['size'])
    features.append(sample['general']['vsize'])
    features.append(sample['general']['has_debug'])
    features.append(sample['general']['exports'])
    features.append(sample['general']['imports'])
    features.append(sample['general']['has_relocations'])
    features.append(sample['general']['has_resources'])
    features.append(sample['general']['has_signature'])
    features.append(sample['general']['has_tls'])
    features.append(sample['general']['symbols'])
    return features

# Step 3: Load Data Incrementally (Limit to 400,000 Samples)
def load_data(file_list, max_samples=400000):
    X, y = [], []
    total_samples = 0
    for file in file_list:
        file_path = os.path.join(data_path, file)
        print(f"Processing {file}...")
        with open(file_path, 'r') as f:
            for line in f:
                if total_samples >= max_samples:
                    break
                sample = json.loads(line.strip())
                if 'label' in sample and sample['label'] in [0, 1]:
                    X.append(extract_features(sample))
                    y.append(sample['label'])
                    total_samples += 1
        if total_samples >= max_samples:
            break
    return np.array(X), np.array(y)

train_files = [f'train_features_{i}.jsonl' for i in range(6)]
test_file = ['test_features.jsonl']

X_train, y_train = load_data(train_files, max_samples=400000)
X_test, y_test = load_data(test_file, max_samples=80000)  # 20% of train size

# Step 4: Train XGBoost Model
print("Training XGBoost model...")
xgb_model = xgb.XGBClassifier(
    objective='binary:logistic',
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1
)
xgb_model.fit(X_train, y_train)

# Step 5: Evaluate Model
y_pred = xgb_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Model 1 Accuracy on Clean Test Data: {accuracy * 100:.2f}%")

# Step 6: Save Model and Datasets
xgb_model.save_model('/content/drive/My Drive/AI_Security_Project/xgb_model_1.json')
np.save('/content/drive/My Drive/AI_Security_Project/X_train_clean.npy', X_train)
np.save('/content/drive/My Drive/AI_Security_Project/y_train_clean.npy', y_train)
np.save('/content/drive/My Drive/AI_Security_Project/X_test.npy', X_test)
np.save('/content/drive/My Drive/AI_Security_Project/y_test.npy', y_test)
print("Model and datasets saved to Drive.")